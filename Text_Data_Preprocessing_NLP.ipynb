{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j-g6b-UBzl8I"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "It is 10 years since Andhra Pradesh was divided into two States.\n",
        "A decade is a long enough time examine the political, economic and historical implications of the division of the political geography of the Telugu people, for them as well as for the Indian Republic.\n",
        "\n",
        "The vitriol that dominated the bifurcation discourse for almost half a decade prior to the actual bifurcation has now vanished without trace. The two successor states, Andhra Pradesh and Telangana, are moving on.\n",
        "Telugu society today on both sides of the political division carries on with very little nostalgia for the nearly five and a half decades of living together in one political entity.\n",
        "For people from the shrunken Andhra Pradesh part, only Hyderabad from the new state of Telangana remains in their imagination.\n",
        "The rest of the geography of Telangana hardly figures in their consciousness. And, for the people of Telangana, no area or aspect of life from across the Andhra Pradesh side of the divide animates their political, social, cultural, or economic imagination.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5KIegNzm0WY9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "id": "4W3CtD200umg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a173a442-20cc-439a-d225-ca5dd3e01274"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "It is 10 years since Andhra Pradesh was divided into two States.\n",
            "A decade is a long enough time examine the political, economic and historical implications of the division of the political geography of the Telugu people, for them as well as for the Indian Republic.\n",
            "\n",
            "The vitriol that dominated the bifurcation discourse for almost half a decade prior to the actual bifurcation has now vanished without trace. The two successor states, Andhra Pradesh and Telangana, are moving on.\n",
            "Telugu society today on both sides of the political division carries on with very little nostalgia for the nearly five and a half decades of living together in one political entity.\n",
            "For people from the shrunken Andhra Pradesh part, only Hyderabad from the new state of Telangana remains in their imagination.\n",
            "The rest of the geography of Telangana hardly figures in their consciousness. And, for the people of Telangana, no area or aspect of life from across the Andhra Pradesh side of the divide animates their political, social, cultural, or economic imagination.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "JcQIh60fFPHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Tokenization"
      ],
      "metadata": {
        "id": "LGFtprhrIO3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "cvIN6BINFQ1A",
        "outputId": "b0f09198-51fc-4e07-fcb0-1a65e1996a52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ce7f38499a01>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GZS2fFrFaFW",
        "outputId": "c5b5418a-389b-44a4-9d1b-111cc1c47d3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = sent_tokenize(corpus)\n",
        "print(type(documents))\n",
        "print()\n",
        "num = 1\n",
        "for sentence in documents:\n",
        "  print(num,sentence)\n",
        "  num += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7XP5IB3FhIV",
        "outputId": "4a25bf24-f597-493c-80ca-29bcda188e3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "\n",
            "1 \n",
            "It is 10 years since Andhra Pradesh was divided into two States.\n",
            "2 A decade is a long enough time examine the political, economic and historical implications of the division of the political geography of the Telugu people, for them as well as for the Indian Republic.\n",
            "3 The vitriol that dominated the bifurcation discourse for almost half a decade prior to the actual bifurcation has now vanished without trace.\n",
            "4 The two successor states, Andhra Pradesh and Telangana, are moving on.\n",
            "5 Telugu society today on both sides of the political division carries on with very little nostalgia for the nearly five and a half decades of living together in one political entity.\n",
            "6 For people from the shrunken Andhra Pradesh part, only Hyderabad from the new state of Telangana remains in their imagination.\n",
            "7 The rest of the geography of Telangana hardly figures in their consciousness.\n",
            "8 And, for the people of Telangana, no area or aspect of life from across the Andhra Pradesh side of the divide animates their political, social, cultural, or economic imagination.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Tokenize"
      ],
      "metadata": {
        "id": "o9g9sdA6IVTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wordpunct_tokenize\n",
        "\n",
        "#### Separation of Punctuation and Words:\n",
        "\n",
        "wordpunct_tokenize is designed to split text into words and punctuation separately. For example, the sentence \"Hello, world!\" would be tokenized as ['Hello', ',', 'world', '!']. This can be useful when you need a clear distinction between words and punctuation for further processing.\n",
        "\n",
        "#### Simplicity:\n",
        "\n",
        "The implementation of wordpunct_tokenize is simpler and based on regular expressions. This can make it faster and more predictable in some cases where you specifically need to separate punctuation from words."
      ],
      "metadata": {
        "id": "Y0GECZadKHDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### word_tokenize\n",
        "\n",
        "#### Context Awareness:\n",
        "\n",
        "word_tokenize uses the Punkt tokenizer, which is more sophisticated and takes context into account. It is capable of handling more complex tokenization tasks, such as recognizing abbreviations, decimal numbers, and multi-word expressions.\n",
        "#### Handling Complex Text:\n",
        "\n",
        "Because it is context-aware, word_tokenize is generally better at handling a wider variety of text inputs, including different sentence structures and more complex punctuation scenarios.\n",
        "#### Language Support:\n",
        "\n",
        "The Punkt tokenizer (used by word_tokenize) has been trained on various languages, making it more versatile for multilingual tokenization tasks."
      ],
      "metadata": {
        "id": "7T_NgePzKa8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "- Use wordpunct_tokenize when you need a straightforward and fast way to separate words from punctuation. It is ideal for tasks where punctuation needs to be treated as separate tokens.\n",
        "- Use word_tokenize when dealing with more complex text that requires context-aware tokenization, especially for more advanced NLP tasks."
      ],
      "metadata": {
        "id": "mUeZytpwKrIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wordpunc_tokenize"
      ],
      "metadata": {
        "id": "uSl3uGHELjGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  words = wordpunct_tokenize(sentence)\n",
        "  print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-TqnzzFLB-1",
        "outputId": "e87462e4-1894-4dcf-e1ef-838d8a06679f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'is', '10', 'years', 'since', 'Andhra', 'Pradesh', 'was', 'divided', 'into', 'two', 'States', '.']\n",
            "['A', 'decade', 'is', 'a', 'long', 'enough', 'time', 'examine', 'the', 'political', ',', 'economic', 'and', 'historical', 'implications', 'of', 'the', 'division', 'of', 'the', 'political', 'geography', 'of', 'the', 'Telugu', 'people', ',', 'for', 'them', 'as', 'well', 'as', 'for', 'the', 'Indian', 'Republic', '.']\n",
            "['The', 'vitriol', 'that', 'dominated', 'the', 'bifurcation', 'discourse', 'for', 'almost', 'half', 'a', 'decade', 'prior', 'to', 'the', 'actual', 'bifurcation', 'has', 'now', 'vanished', 'without', 'trace', '.']\n",
            "['The', 'two', 'successor', 'states', ',', 'Andhra', 'Pradesh', 'and', 'Telangana', ',', 'are', 'moving', 'on', '.']\n",
            "['Telugu', 'society', 'today', 'on', 'both', 'sides', 'of', 'the', 'political', 'division', 'carries', 'on', 'with', 'very', 'little', 'nostalgia', 'for', 'the', 'nearly', 'five', 'and', 'a', 'half', 'decades', 'of', 'living', 'together', 'in', 'one', 'political', 'entity', '.']\n",
            "['For', 'people', 'from', 'the', 'shrunken', 'Andhra', 'Pradesh', 'part', ',', 'only', 'Hyderabad', 'from', 'the', 'new', 'state', 'of', 'Telangana', 'remains', 'in', 'their', 'imagination', '.']\n",
            "['The', 'rest', 'of', 'the', 'geography', 'of', 'Telangana', 'hardly', 'figures', 'in', 'their', 'consciousness', '.']\n",
            "['And', ',', 'for', 'the', 'people', 'of', 'Telangana', ',', 'no', 'area', 'or', 'aspect', 'of', 'life', 'from', 'across', 'the', 'Andhra', 'Pradesh', 'side', 'of', 'the', 'divide', 'animates', 'their', 'political', ',', 'social', ',', 'cultural', ',', 'or', 'economic', 'imagination', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word_tokenize"
      ],
      "metadata": {
        "id": "frR9ioTXLmxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_words = []\n",
        "for sentence in documents:\n",
        "  words = word_tokenize(sentence)\n",
        "  print()\n",
        "  print(words)\n",
        "  document_words.append(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEGM7ypdFlTz",
        "outputId": "ba9a06d0-8086-4dbc-af39-30f3cd6e65bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['It', 'is', '10', 'years', 'since', 'Andhra', 'Pradesh', 'was', 'divided', 'into', 'two', 'States', '.']\n",
            "\n",
            "['A', 'decade', 'is', 'a', 'long', 'enough', 'time', 'examine', 'the', 'political', ',', 'economic', 'and', 'historical', 'implications', 'of', 'the', 'division', 'of', 'the', 'political', 'geography', 'of', 'the', 'Telugu', 'people', ',', 'for', 'them', 'as', 'well', 'as', 'for', 'the', 'Indian', 'Republic', '.']\n",
            "\n",
            "['The', 'vitriol', 'that', 'dominated', 'the', 'bifurcation', 'discourse', 'for', 'almost', 'half', 'a', 'decade', 'prior', 'to', 'the', 'actual', 'bifurcation', 'has', 'now', 'vanished', 'without', 'trace', '.']\n",
            "\n",
            "['The', 'two', 'successor', 'states', ',', 'Andhra', 'Pradesh', 'and', 'Telangana', ',', 'are', 'moving', 'on', '.']\n",
            "\n",
            "['Telugu', 'society', 'today', 'on', 'both', 'sides', 'of', 'the', 'political', 'division', 'carries', 'on', 'with', 'very', 'little', 'nostalgia', 'for', 'the', 'nearly', 'five', 'and', 'a', 'half', 'decades', 'of', 'living', 'together', 'in', 'one', 'political', 'entity', '.']\n",
            "\n",
            "['For', 'people', 'from', 'the', 'shrunken', 'Andhra', 'Pradesh', 'part', ',', 'only', 'Hyderabad', 'from', 'the', 'new', 'state', 'of', 'Telangana', 'remains', 'in', 'their', 'imagination', '.']\n",
            "\n",
            "['The', 'rest', 'of', 'the', 'geography', 'of', 'Telangana', 'hardly', 'figures', 'in', 'their', 'consciousness', '.']\n",
            "\n",
            "['And', ',', 'for', 'the', 'people', 'of', 'Telangana', ',', 'no', 'area', 'or', 'aspect', 'of', 'life', 'from', 'across', 'the', 'Andhra', 'Pradesh', 'side', 'of', 'the', 'divide', 'animates', 'their', 'political', ',', 'social', ',', 'cultural', ',', 'or', 'economic', 'imagination', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fpU89ClNIp9f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# stopwords"
      ],
      "metadata": {
        "id": "bT9DroSjM-vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = stopwords.words('eng')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lvm934JHNDDn",
        "outputId": "419603ec-e94f-4aa5-d14d-f0491ffd328c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-74101a4dc61e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eng'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONuGzWs9NXy8",
        "outputId": "e8d31f64-4701-40f4-ebed-663fbaf93c9d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "for sentence in documents:\n",
        "  tokens = word_tokenize(sentence)\n",
        "  print(\"The number of words in the document before applying stopwords \",len(tokens))\n",
        "  print()\n",
        "  process_tokens = [words for words in tokens if words.lower() and words not in stop_words]\n",
        "  print(process_tokens)\n",
        "  print(\"The number of words in the document after applying stopwords \",len(process_tokens))\n",
        "  print()\n",
        "  print(\"=\"*150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fL1y0RENmBp",
        "outputId": "132ce0ed-56f9-4767-a6f4-44190e553557"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of words in the document before applying stopwords  13\n",
            "\n",
            "['It', '10', 'years', 'since', 'Andhra', 'Pradesh', 'divided', 'two', 'States', '.']\n",
            "The number of words in the document after applying stopwords  10\n",
            "\n",
            "======================================================================================================================================================\n",
            "The number of words in the document before applying stopwords  37\n",
            "\n",
            "['A', 'decade', 'long', 'enough', 'time', 'examine', 'political', ',', 'economic', 'historical', 'implications', 'division', 'political', 'geography', 'Telugu', 'people', ',', 'well', 'Indian', 'Republic', '.']\n",
            "The number of words in the document after applying stopwords  21\n",
            "\n",
            "======================================================================================================================================================\n",
            "The number of words in the document before applying stopwords  23\n",
            "\n",
            "['The', 'vitriol', 'dominated', 'bifurcation', 'discourse', 'almost', 'half', 'decade', 'prior', 'actual', 'bifurcation', 'vanished', 'without', 'trace', '.']\n",
            "The number of words in the document after applying stopwords  15\n",
            "\n",
            "======================================================================================================================================================\n",
            "The number of words in the document before applying stopwords  14\n",
            "\n",
            "['The', 'two', 'successor', 'states', ',', 'Andhra', 'Pradesh', 'Telangana', ',', 'moving', '.']\n",
            "The number of words in the document after applying stopwords  11\n",
            "\n",
            "======================================================================================================================================================\n",
            "The number of words in the document before applying stopwords  32\n",
            "\n",
            "['Telugu', 'society', 'today', 'sides', 'political', 'division', 'carries', 'little', 'nostalgia', 'nearly', 'five', 'half', 'decades', 'living', 'together', 'one', 'political', 'entity', '.']\n",
            "The number of words in the document after applying stopwords  19\n",
            "\n",
            "======================================================================================================================================================\n",
            "The number of words in the document before applying stopwords  22\n",
            "\n",
            "['For', 'people', 'shrunken', 'Andhra', 'Pradesh', 'part', ',', 'Hyderabad', 'new', 'state', 'Telangana', 'remains', 'imagination', '.']\n",
            "The number of words in the document after applying stopwords  14\n",
            "\n",
            "======================================================================================================================================================\n",
            "The number of words in the document before applying stopwords  13\n",
            "\n",
            "['The', 'rest', 'geography', 'Telangana', 'hardly', 'figures', 'consciousness', '.']\n",
            "The number of words in the document after applying stopwords  8\n",
            "\n",
            "======================================================================================================================================================\n",
            "The number of words in the document before applying stopwords  35\n",
            "\n",
            "['And', ',', 'people', 'Telangana', ',', 'area', 'aspect', 'life', 'across', 'Andhra', 'Pradesh', 'side', 'divide', 'animates', 'political', ',', 'social', ',', 'cultural', ',', 'economic', 'imagination', '.']\n",
            "The number of words in the document after applying stopwords  23\n",
            "\n",
            "======================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "txt = \"Sukanya, Rajib and Naba are my good friends. \" \\\n",
        "\t\"Sukanya is getting married next year. \" \\\n",
        "\t\"Marriage is a big step in one’s life.\" \\\n",
        "\t\"It is both exciting and frightening. \" \\\n",
        "\t\"But friendship is a sacred bond between people.\" \\\n",
        "\t\"It is a special kind of love between us. \" \\\n",
        "\t\"Many of you must have tried searching for a friend \"\\\n",
        "\t\"but never found the right one.\"\n",
        "\n",
        "# sent_tokenize is one of instances of\n",
        "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
        "\n",
        "tokenized = sent_tokenize(txt)\n",
        "for i in tokenized:\n",
        "\n",
        "\t# Word tokenizers is used to find the words\n",
        "\t# and punctuation in a string\n",
        "\twordsList = nltk.word_tokenize(i)\n",
        "\n",
        "\t# removing stop words from wordList\n",
        "\twordsList = [w for w in wordsList if not w in stop_words]\n",
        "\n",
        "\t# Using a Tagger. Which is part-of-speech\n",
        "\t# tagger or POS-tagger.\n",
        "\ttagged = nltk.pos_tag(wordsList)\n",
        "\n",
        "\tprint(tagged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "iCEZ2jH4NnbT",
        "outputId": "14c67750-58b9-4ef5-93e2-8396f2b6479b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2cf450f21db5>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Using a Tagger. Which is part-of-speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# tagger or POS-tagger.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             )\n\u001b[1;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFjGnMVsRS7B",
        "outputId": "dfd138c5-3436-4e40-c227-9b5da1b47950"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "txt = \"Sukanya, Rajib and Naba are my good friends. \" \\\n",
        "\t\"Sukanya is getting married next year. \" \\\n",
        "\t\"Marriage is a big step in one’s life.\" \\\n",
        "\t\"It is both exciting and frightening. \" \\\n",
        "\t\"But friendship is a sacred bond between people.\" \\\n",
        "\t\"It is a special kind of love between us. \" \\\n",
        "\t\"Many of you must have tried searching for a friend \"\\\n",
        "\t\"but never found the right one.\"\n",
        "\n",
        "# sent_tokenize is one of instances of\n",
        "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
        "\n",
        "tokenized = sent_tokenize(txt)\n",
        "for i in tokenized:\n",
        "\n",
        "\t# Word tokenizers is used to find the words\n",
        "\t# and punctuation in a string\n",
        "\twordsList = nltk.word_tokenize(i)\n",
        "\n",
        "\t# removing stop words from wordList\n",
        "\twordsList = [w for w in wordsList if not w in stop_words]\n",
        "\n",
        "\t# Using a Tagger. Which is part-of-speech\n",
        "\t# tagger or POS-tagger.\n",
        "\ttagged = nltk.pos_tag(wordsList)\n",
        "\n",
        "\tprint(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2WZZn7wRVtd",
        "outputId": "4e1d4951-2e42-4a93-c4d7-1d1eab0f731d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Sukanya', 'NNP'), (',', ','), ('Rajib', 'NNP'), ('Naba', 'NNP'), ('good', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n",
            "[('Sukanya', 'NNP'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN'), ('.', '.')]\n",
            "[('Marriage', 'NN'), ('big', 'JJ'), ('step', 'NN'), ('one', 'CD'), ('’', 'NN'), ('life.It', 'NN'), ('exciting', 'VBG'), ('frightening', 'NN'), ('.', '.')]\n",
            "[('But', 'CC'), ('friendship', 'NN'), ('sacred', 'VBD'), ('bond', 'NN'), ('people.It', 'NN'), ('special', 'JJ'), ('kind', 'NN'), ('love', 'VB'), ('us', 'PRP'), ('.', '.')]\n",
            "[('Many', 'JJ'), ('must', 'MD'), ('tried', 'VB'), ('searching', 'VBG'), ('friend', 'NN'), ('never', 'RB'), ('found', 'VBD'), ('right', 'JJ'), ('one', 'CD'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p1qrNvZARZZn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS - Reference"
      ],
      "metadata": {
        "id": "q8IviBcpRmVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CC coordinating conjunction\n",
        "# CD cardinal digit\n",
        "# DT determiner\n",
        "# EX existential there (like: “there is” … think of it like “there exists”)\n",
        "# FW foreign word\n",
        "# IN preposition/subordinating conjunction\n",
        "# JJ adjective – ‘big’\n",
        "# JJR adjective, comparative – ‘bigger’\n",
        "# JJS adjective, superlative – ‘biggest’\n",
        "# LS list marker 1)\n",
        "# MD modal – could, will\n",
        "# NN noun, singular ‘- desk’\n",
        "# NNS noun plural – ‘desks’\n",
        "# NNP proper noun, singular – ‘Harrison’\n",
        "# NNPS proper noun, plural – ‘Americans’\n",
        "# PDT predeterminer – ‘all the kids’\n",
        "# POS possessive ending parent’s\n",
        "# PRP personal pronoun –  I, he, she\n",
        "# PRP$ possessive pronoun – my, his, hers\n",
        "# RB adverb – very, silently,\n",
        "# RBR adverb, comparative – better\n",
        "# RBS adverb, superlative – best\n",
        "# RP particle – give up\n",
        "# TO – to go ‘to’ the store.\n",
        "# UH interjection – errrrrrrrm\n",
        "# VB verb, base form – take\n",
        "# VBD verb, past tense – took\n",
        "# VBG verb, gerund/present participle – taking\n",
        "# VBN verb, past participle – taken\n",
        "# VBP verb, sing. present, non-3d – take\n",
        "# VBZ verb, 3rd person sing. present – takes\n",
        "# WDT wh-determiner – which\n",
        "# WP wh-pronoun – who, what\n",
        "# WP$ possessive wh-pronoun, eg- whose\n",
        "# WRB wh-adverb, eg- where, when"
      ],
      "metadata": {
        "id": "MYCO9Y6cRon9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxoygRvJRqLU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "zV_fLfbFSMI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stem = PorterStemmer().stem"
      ],
      "metadata": {
        "id": "W3DZjnHcSwJG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " plurals = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', 'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', 'traditional', 'reference', 'colonizer', 'plotted']\n",
        " for i in plurals:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtvAxJ7rmwv9",
        "outputId": "14b57ed4-5c1a-4c44-9651-f9705b36d096"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caresses\n",
            "flies\n",
            "dies\n",
            "mules\n",
            "denied\n",
            "died\n",
            "agreed\n",
            "owned\n",
            "humbled\n",
            "sized\n",
            "meeting\n",
            "stating\n",
            "siezing\n",
            "itemization\n",
            "sensational\n",
            "traditional\n",
            "reference\n",
            "colonizer\n",
            "plotted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in plurals:\n",
        "  print(i,\"------------>\",stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF-pPOj-nt-E",
        "outputId": "c7881bbf-df22-4a8c-ba2e-ed112158d270"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caresses ------------> caress\n",
            "flies ------------> fli\n",
            "dies ------------> die\n",
            "mules ------------> mule\n",
            "denied ------------> deni\n",
            "died ------------> die\n",
            "agreed ------------> agre\n",
            "owned ------------> own\n",
            "humbled ------------> humbl\n",
            "sized ------------> size\n",
            "meeting ------------> meet\n",
            "stating ------------> state\n",
            "siezing ------------> siez\n",
            "itemization ------------> item\n",
            "sensational ------------> sensat\n",
            "traditional ------------> tradit\n",
            "reference ------------> refer\n",
            "colonizer ------------> colon\n",
            "plotted ------------> plot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SnowballStemmer"
      ],
      "metadata": {
        "id": "zTj9CzDTpp-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snow_stemmer = SnowballStemmer('english').stem\n",
        "for i in plurals:\n",
        "  print(i,\"------------>\",snow_stemmer(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_2cBOI5n_jK",
        "outputId": "0781e23f-6f1a-4c99-bae3-b811b6745178"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caresses ------------> caress\n",
            "flies ------------> fli\n",
            "dies ------------> die\n",
            "mules ------------> mule\n",
            "denied ------------> deni\n",
            "died ------------> die\n",
            "agreed ------------> agre\n",
            "owned ------------> own\n",
            "humbled ------------> humbl\n",
            "sized ------------> size\n",
            "meeting ------------> meet\n",
            "stating ------------> state\n",
            "siezing ------------> siez\n",
            "itemization ------------> item\n",
            "sensational ------------> sensat\n",
            "traditional ------------> tradit\n",
            "reference ------------> refer\n",
            "colonizer ------------> colon\n",
            "plotted ------------> plot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "60g42Gw4pv1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatization = WordNetLemmatizer()\n",
        "for i in plurals:\n",
        "  print(i,\"------------>\",lemmatization.lemmatize(i))\n",
        "\n",
        "\n",
        "# or\n",
        "\n",
        "\n",
        "\n",
        "# lemmatization = WordNetLemmatizer().lemmatize\n",
        "# for i in plurals:\n",
        "  # print(i,\"------------>\",lemmatize(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qNs2qu6eol1J",
        "outputId": "53765ce1-5df0-40b2-aff6-ec8ba023ea4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-53fba81dbb4b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlemmatization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplurals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"------------>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlemmatization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ETm8lU0paQY",
        "outputId": "ab46a451-c018-4eb7-ddae-dcc3d6bef49c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in plurals:\n",
        "  print(i,\"------------>\",lemmatization.lemmatize(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBbIrarcpcsX",
        "outputId": "9ec86a45-2e1a-4655-b139-14036196c6c7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caresses ------------> caress\n",
            "flies ------------> fly\n",
            "dies ------------> dy\n",
            "mules ------------> mule\n",
            "denied ------------> denied\n",
            "died ------------> died\n",
            "agreed ------------> agreed\n",
            "owned ------------> owned\n",
            "humbled ------------> humbled\n",
            "sized ------------> sized\n",
            "meeting ------------> meeting\n",
            "stating ------------> stating\n",
            "siezing ------------> siezing\n",
            "itemization ------------> itemization\n",
            "sensational ------------> sensational\n",
            "traditional ------------> traditional\n",
            "reference ------------> reference\n",
            "colonizer ------------> colonizer\n",
            "plotted ------------> plotted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kDLLe3Hpib1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming and Lemmatizing the corpus"
      ],
      "metadata": {
        "id": "29AV6sWRqI5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentances = \" \"\n",
        "for sentence in documents:\n",
        "  words_list =  word_tokenize(sentence)\n",
        "  processed_words_list = [lemmatization.lemmatize(word)  for word in words_list if word.lower() and word not in stop_words]\n",
        "  print(processed_words_list)\n",
        "  sentances = \" \".join(processed_words_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPVWTBTUqQKt",
        "outputId": "98908c8a-ec3f-4ca8-cd77-8aea17b22d57"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', '10', 'year', 'since', 'Andhra', 'Pradesh', 'divided', 'two', 'States', '.']\n",
            "['A', 'decade', 'long', 'enough', 'time', 'examine', 'political', ',', 'economic', 'historical', 'implication', 'division', 'political', 'geography', 'Telugu', 'people', ',', 'well', 'Indian', 'Republic', '.']\n",
            "['The', 'vitriol', 'dominated', 'bifurcation', 'discourse', 'almost', 'half', 'decade', 'prior', 'actual', 'bifurcation', 'vanished', 'without', 'trace', '.']\n",
            "['The', 'two', 'successor', 'state', ',', 'Andhra', 'Pradesh', 'Telangana', ',', 'moving', '.']\n",
            "['Telugu', 'society', 'today', 'side', 'political', 'division', 'carry', 'little', 'nostalgia', 'nearly', 'five', 'half', 'decade', 'living', 'together', 'one', 'political', 'entity', '.']\n",
            "['For', 'people', 'shrunken', 'Andhra', 'Pradesh', 'part', ',', 'Hyderabad', 'new', 'state', 'Telangana', 'remains', 'imagination', '.']\n",
            "['The', 'rest', 'geography', 'Telangana', 'hardly', 'figure', 'consciousness', '.']\n",
            "['And', ',', 'people', 'Telangana', ',', 'area', 'aspect', 'life', 'across', 'Andhra', 'Pradesh', 'side', 'divide', 'animates', 'political', ',', 'social', ',', 'cultural', ',', 'economic', 'imagination', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkz84JuJrhOC",
        "outputId": "466f83d8-165d-4dcb-b837-5ba9c226d5b5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And , people Telangana , area aspect life across Andhra Pradesh side divide animates political , social , cultural , economic imagination .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = sent_tokenize(corpus)\n",
        "\n",
        "for sentence in range(len(documents)):\n",
        "  words_list =  word_tokenize(documents[sentence])\n",
        "  processed_words_list = [lemmatization.lemmatize(word)  for word in words_list if word.lower() and word not in stop_words]\n",
        "  print(processed_words_list)\n",
        "  documents[sentence] = \" \".join(processed_words_list)\n",
        "  print()\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peU9dCyHr69z",
        "outputId": "cae04c59-6d9d-4223-ec11-f05bd809fd77"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', '10', 'year', 'since', 'Andhra', 'Pradesh', 'divided', 'two', 'States', '.']\n",
            "\n",
            "['A', 'decade', 'long', 'enough', 'time', 'examine', 'political', ',', 'economic', 'historical', 'implication', 'division', 'political', 'geography', 'Telugu', 'people', ',', 'well', 'Indian', 'Republic', '.']\n",
            "\n",
            "['The', 'vitriol', 'dominated', 'bifurcation', 'discourse', 'almost', 'half', 'decade', 'prior', 'actual', 'bifurcation', 'vanished', 'without', 'trace', '.']\n",
            "\n",
            "['The', 'two', 'successor', 'state', ',', 'Andhra', 'Pradesh', 'Telangana', ',', 'moving', '.']\n",
            "\n",
            "['Telugu', 'society', 'today', 'side', 'political', 'division', 'carry', 'little', 'nostalgia', 'nearly', 'five', 'half', 'decade', 'living', 'together', 'one', 'political', 'entity', '.']\n",
            "\n",
            "['For', 'people', 'shrunken', 'Andhra', 'Pradesh', 'part', ',', 'Hyderabad', 'new', 'state', 'Telangana', 'remains', 'imagination', '.']\n",
            "\n",
            "['The', 'rest', 'geography', 'Telangana', 'hardly', 'figure', 'consciousness', '.']\n",
            "\n",
            "['And', ',', 'people', 'Telangana', ',', 'area', 'aspect', 'life', 'across', 'Andhra', 'Pradesh', 'side', 'divide', 'animates', 'political', ',', 'social', ',', 'cultural', ',', 'economic', 'imagination', '.']\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It 10 year since Andhra Pradesh divided two States .',\n",
              " 'A decade long enough time examine political , economic historical implication division political geography Telugu people , well Indian Republic .',\n",
              " 'The vitriol dominated bifurcation discourse almost half decade prior actual bifurcation vanished without trace .',\n",
              " 'The two successor state , Andhra Pradesh Telangana , moving .',\n",
              " 'Telugu society today side political division carry little nostalgia nearly five half decade living together one political entity .',\n",
              " 'For people shrunken Andhra Pradesh part , Hyderabad new state Telangana remains imagination .',\n",
              " 'The rest geography Telangana hardly figure consciousness .',\n",
              " 'And , people Telangana , area aspect life across Andhra Pradesh side divide animates political , social , cultural , economic imagination .']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nUl5-k2EufdZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}